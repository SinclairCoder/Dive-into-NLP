{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "uuid": "58ac99d1-7fcc-4aec-a5ba-56ba9bf07018"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 15:12:51,503 INFO: Use cuda: True, gpu id: 0.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random \n",
    "import numpy as np\n",
    "import transformers\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 2020\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "gpu = 0\n",
    "use_cuda = gpu>=0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\",gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "uuid": "1ff1c19d-168a-4fa5-bb1f-b1eb07715503"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 15:51:56,931 INFO: Fold lens [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]\n"
     ]
    }
   ],
   "source": [
    "fold_num = 10\n",
    "DATA_PATH = 'train_set.csv'\n",
    "import pandas as pd\n",
    "\n",
    "def all_data2fold(fold_num,num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(DATA_PATH,sep='\\t',encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "    \n",
    "    total = len(labels)\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label,data in label2id.items():\n",
    "        batch_size = int(len(data)/fold_num)\n",
    "        other = len(data) - batch_size*fold_num\n",
    "        pre_batch_size = batch_size\n",
    "        pre_end = 0\n",
    "        for i in range(fold_num):\n",
    "            if i < other:\n",
    "                cur_batch_size = batch_size +1;\n",
    "            else: cur_batch_size = batch_size\n",
    "            batch_data = [data[pre_end+b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "            pre_batch_size = cur_batch_size\n",
    "            pre_end += cur_batch_size\n",
    "    \n",
    "    batch_size = int(total/fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "        \n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num +=num-batch_size\n",
    "        # 实现对orther_texts的利用\n",
    "        elif num<batch_size:\n",
    "            end = start+batch_size-num\n",
    "            fold_texts = texts + other_texts[start:end]\n",
    "            fold_labels = labels + other_labels[start:end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "        assert batch_size == len(fold_labels)\n",
    "        \n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "        data = {'label':shuffle_fold_labels,'text':shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "    return fold_data\n",
    "\n",
    "fold_data = all_data2fold(10,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "uuid": "4a327b87-0426-4119-958d-134473fd36e4"
   },
   "outputs": [],
   "source": [
    "fold_id = 9\n",
    "# 验证集\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# 训练集\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in range(0,fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "train_data = {'label':train_labels,'text':train_texts}\n",
    "\n",
    "# 测试集\n",
    "test_data_file = 'test_a.csv'\n",
    "f = pd.read_csv(test_data_file,sep='\\t',encoding='UTF-8')\n",
    "texts = f['text'].tolist()\n",
    "test_data = {'label':[0]*len(texts),'text':texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "uuid": "c8a5933c-6de2-40c5-89a6-71de5eb502be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-04 15:52:51,065 INFO: Bulid vocab: word 5503, labels 14.\n"
     ]
    }
   ],
   "source": [
    "# bulid vocab\n",
    "from collections import Counter\n",
    "from transformers import BasicTokenizer\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self,train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]','[UNK]']\n",
    "        self._id2extword = ['[PAD]','[UNK]']\n",
    "        \n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "        self.build_vocab(train_data)\n",
    "        \n",
    "        reverse = lambda x: dict(zip(x,range(len(x)) ))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "        \n",
    "        logging.info(\"Bulid vocab: word %d, labels %d.\"%(self.word_size,self.label_size))\n",
    "        \n",
    "    def build_vocab(self,data):\n",
    "        self.word_counter = Counter()\n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word]+= 1\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count> self.min_count:\n",
    "                self._id2word.append(word)\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "        self.label_counter = Counter(data['label'])\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "    def load_pretrained_embs(self,embfile):\n",
    "        with open(embfile,encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            items = lines[0].split()\n",
    "            word_count,embedding_dim = int(items[0]) ,int(items[1])\n",
    "        index = len(self._id2extword)\n",
    "        embeddings = np.zeros((word_count+index,embedding_dim))\n",
    "        for line in lines[1:]:\n",
    "            values = line.split()\n",
    "            self._id2extword.append(values[0])\n",
    "            vector = np.array(values[1:],dtype='float64')\n",
    "            embeddings[self.unk] += vector\n",
    "            embeddings[index] = vector\n",
    "            index += 1\n",
    "        # unknow token的embedding 就是把整个词表的embedding 平均了一下\n",
    "        embeddings[self.unk] = embedding[self.unk]//word_count\n",
    "        embeddings = embeddings /np.std(embeddings)\n",
    "        \n",
    "        reverse = lambda x: dict(zip(x,range(len(x))))\n",
    "        self._extword2id = reverse(self._id2extword)\n",
    "        \n",
    "        assert len(set(self._id2extword)) == len(self._id2extword)\n",
    "        \n",
    "        return embeddings\n",
    "    # 字典(Dictionary) get() 函数返回指定键的值，如果值不在字典中返回默认值。\n",
    "    def word2id(self,xs):\n",
    "        if ininstance(xs,list):\n",
    "            return [self._word2id.get(x,self.unk) for x in xs]\n",
    "        return self._word2id.get(xs,self.unk)\n",
    "    def extword2id(self,xs):\n",
    "        if isinstance(xs,list):\n",
    "            return [self._extword2id.get(x,self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs,self.unk)\n",
    "    def label2id(self,xs):\n",
    "        if isinstance(xs,list):\n",
    "            return [self._label2id.get(x,self.unk) for x in xs]\n",
    "        return self._label2id.get(xs,self.unk)\n",
    "    \n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)\n",
    "\n",
    "vocab = Vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "uuid": "afc05e83-ed13-42a5-a407-47e553d2acae"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super(Attention,self)._init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.weight.data.normal_(mean=0.0,std=0.05)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b = np.zeros(hidden_size,dtype=np.float32)\n",
    "        self.bias.data.copy_(torch.from_numpy)\n",
    "        self.query = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.query = nn.data.normal_(mean=0.0,std=0.05)\n",
    "    def forward(self,batch_hidden,batch_masks):\n",
    "        \n",
    "        # key 其中matmul针对高位矩阵相乘\n",
    "        key = torch.matmul(batch_hidden,self.weight) +self.bias\n",
    "        \n",
    "        # compute attention\n",
    "        outputs = torch.matmul(key,self.query)\n",
    "        \n",
    "        masked_outputs = outputs.masked_fill((1-batch_masks).bool(),float(-1e32))\n",
    "        \n",
    "        attention_scores = F.softmax(masked_outputs,dim=1)\n",
    "        \n",
    "        masked_attention_scores = attention_scores.masked_fill((1-batch_masks).bool(),0.0)\n",
    "        \n",
    "        batch_outputs = torch.bmm(masked_attention_scores.unsequeeze(1),key).squeeze(1)\n",
    "        \n",
    "        return batch_outputs, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "af6624a9-9337-489f-a659-669f80646f1c"
   },
   "outputs": [],
   "source": [
    "word2vec_path = 'word2vec.txt'\n",
    "drop_out = 0.15\n",
    "\n",
    "class WordCNNEncoder(nn.Module):\n",
    "    def __init__(self,vocab):\n",
    "        super(WordCNNEncoder,self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.word_dims = 100\n",
    "        \n",
    "        self.word_embed = nn.Embedding(vocab.word_size,self.word_dims,padding_idx=0)\n",
    "        \n",
    "        extword_embed = vocab.load_pretrained_embs(word2vec_path)\n",
    "        extword_size, word_dims = extword_embed.shape\n",
    "        logging.info(\"Load extword embed: words %d, dims %d.\"%(extword_size,word_dims))\n",
    "        \n",
    "        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=0)\n",
    "        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))\n",
    "        self.extword_embed.weigth.requires_grad = False\n",
    "        \n",
    "        input_size = self.word_dims\n",
    "        # n_gram window\n",
    "        self.filter_sizes = [2,3,4]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
